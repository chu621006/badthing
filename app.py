import streamlit as st
import pandas as pd
import pdfplumber
import collections
import re
import io

from PIL import Image
import pytesseract

# **æ˜ç¢ºæŒ‡å®š tesseract åŸ·è¡Œæª”çš„è·¯å¾‘**
pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'

# å°å…¥ img2table çš„ PDF é¡ å’Œ TesseractOCR
from img2table.document import PDF as Img2TablePDF
from img2table.ocr import TesseractOCR

# --- è¼”åŠ©å‡½æ•¸ ---
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    å°‡å¤šå€‹ç©ºç™½å­—å…ƒï¼ˆåŒ…æ‹¬æ›è¡Œï¼‰æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å…©ç«¯ç©ºç™½ã€‚
    """
    if cell_content is None:
        return ""

    text = ""
    if hasattr(cell_content, 'text'):
        text = str(cell_content.text)
    elif isinstance(cell_content, str):
        text = cell_content
    else:
        text = str(cell_content)
    
    return re.sub(r'\s+', ' ', text).strip()

def make_unique_columns(columns_list):
    """
    å°‡åˆ—è¡¨ä¸­çš„æ¬„ä½åç¨±è½‰æ›ç‚ºå”¯ä¸€çš„åç¨±ï¼Œè™•ç†é‡è¤‡å’Œç©ºå­—ä¸²ã€‚
    å¦‚æœé‡åˆ°é‡è¤‡æˆ–ç©ºå­—ä¸²ï¼Œæœƒæ·»åŠ å¾Œç¶´ (ä¾‹å¦‚ 'Column_1', 'æ¬„ä½_2')ã€‚
    """
    seen = collections.defaultdict(int)
    unique_columns = []
    for col in columns_list:
        original_col = col
        if not col.strip():  # å¦‚æœæ¬„ä½æ˜¯ç©ºå­—ä¸²ï¼Œçµ¦ä¸€å€‹é è¨­åç¨±
            col = "ç©ºç™½æ¬„ä½"
        
        if col in seen:
            seen[col] += 1
            unique_columns.append(f"{col}_{seen[col]}")
        else:
            seen[col] = 0
            unique_columns.append(col)
    return unique_columns

# **ä¿®æ”¹å¾Œçš„ process_pdf_file å‡½æ•¸ï¼šç›´æ¥èª¿ç”¨ img2table**
def process_pdf_file(uploaded_file):
    """
    ç›´æ¥ä½¿ç”¨ img2table + OCR è™•ç† PDF æª”æ¡ˆï¼Œæå–è¡¨æ ¼æ•¸æ“šã€‚
    """
    st.info("ç›´æ¥ä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼ (é©ç”¨æ–¼åœ–ç‰‡å‹æˆ–ç„¡é‚Šç•Œè¡¨æ ¼)...")
    try:
        df = process_image_pdf_with_ocr(uploaded_file)
        if df.empty:
            st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆè¡¨æ ¼æ•¸æ“šã€‚")
        return df
    except Exception as e:
        st.error(f"è™•ç† PDF æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹ç¢ºä¿ Tesseract OCR å¼•æ“åŠæ‰€æœ‰ç›¸é—œä¾è³´å·²æ­£ç¢ºå®‰è£ã€‚")
        return pd.DataFrame()

def process_image_pdf_with_ocr(uploaded_file):
    """
    ä½¿ç”¨ img2table + Tesseract OCR è™•ç†åœ–ç‰‡ PDFï¼Œæå–è¡¨æ ¼æ•¸æ“šã€‚
    """
    st.info("æ­£åœ¨ä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼...")
    try:
        pdf_bytes = io.BytesIO(uploaded_file.read())

        ocr = TesseractOCR(lang="chi_tra")

        doc = Img2TablePDF(src=pdf_bytes, pages="all")

        # é€™è£¡å¯ä»¥èª¿æ•´ img2table çš„åƒæ•¸ä¾†æé«˜è­˜åˆ¥ç‡
        # implicit_rows: å¦‚æœè¡¨æ ¼æ²’æœ‰æ˜ç¢ºè¡Œï¼Œæ˜¯å¦å˜—è©¦è­˜åˆ¥éš±å«è¡Œ
        # borderless_tables: æ˜¯å¦å˜—è©¦è­˜åˆ¥ç„¡é‚Šç•Œè¡¨æ ¼ (å°æ‚¨é€™ç¨®PDFå¾ˆé‡è¦)
        extracted_tables = doc.extract_tables(ocr=ocr,
                                              implicit_rows=True, # <--- å˜—è©¦è¨­ç‚º True
                                              borderless_tables=True, # <--- è¨­ç‚º Trueï¼Œå°æ–¼ç„¡é‚Šç•Œè¡¨æ ¼å¾ˆé‡è¦
                                              min_confidence=70) # <--- å¯ä»¥é©ç•¶é™ä½æˆ–ä¿æŒï¼Œçœ‹æ•ˆæœ

        if extracted_tables:
            all_dfs = []
            for page_num, tables_on_page in extracted_tables.items():
                for idx, table in enumerate(tables_on_page):
                    df = pd.DataFrame(table.content)
                    all_dfs.append(df)

            if all_dfs:
                merged_df = pd.concat(all_dfs, ignore_index=True)
                # å°æ‰€æœ‰å–®å…ƒæ ¼å…§å®¹é€²è¡Œæ¨™æº–åŒ–
                merged_df = merged_df.applymap(normalize_text)

                # **æ”¹é€²åˆ—åè™•ç†ï¼šå†æ¬¡çµ±ä¸€è™•ç†ï¼Œä¸¦å‡è¨­ç¬¬ä¸€è¡Œæ˜¯åˆ—å**
                if not merged_df.empty:
                    # å˜—è©¦å°‡ç¬¬ä¸€è¡Œä½œç‚ºåˆ—å
                    header = merged_df.iloc[0].tolist()
                    header = [normalize_text(h) for h in header] # å†æ¬¡æ¨™æº–åŒ–è¡¨é ­
                    header = make_unique_columns(header) # ç¢ºä¿åˆ—åå”¯ä¸€
                    
                    if len(header) == merged_df.shape[1]: # æª¢æŸ¥åˆ—æ•¸æ˜¯å¦åŒ¹é…
                        merged_df.columns = header
                        merged_df = merged_df[1:].reset_index(drop=True) # ç§»é™¤è¡¨é ­è¡Œ
                    else:
                        st.warning("æå–åˆ°çš„åˆ—æ•¸èˆ‡è¡¨é ­ä¸åŒ¹é…ï¼Œå¯èƒ½å°è‡´åˆ—åéŒ¯èª¤ã€‚")
                        # å¦‚æœä¸åŒ¹é…ï¼Œå°±ä¿ç•™åŸä¾†çš„æ•¸å­—åˆ—åï¼Œè®“ä½¿ç”¨è€…æª¢æŸ¥
                
                st.success("æˆåŠŸä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼ã€‚")
                return merged_df
            else:
                st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚")
                return pd.DataFrame()
        else:
            st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚")
            return pd.DataFrame()

    except Exception as e:
        st.error(f"ä½¿ç”¨ img2table è™•ç†åœ–ç‰‡ PDF æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹ç¢ºä¿ Dockerfile æ­£ç¢ºå®‰è£äº† Tesseract OCR å¼•æ“å’Œæ‰€æœ‰ç›¸é—œçš„ç³»çµ±ä¾è³´ï¼ˆå¦‚ libGL.so.1 ç­‰ï¼‰ã€‚")
        return pd.DataFrame()

def parse_course_data(df):
    """
    å¾ DataFrame ä¸­è§£æèª²ç¨‹æ•¸æ“šï¼Œè¨ˆç®—ç¸½å­¸åˆ†å’Œä¸åŠæ ¼å­¸åˆ†ã€‚
    """
    parsed_courses = []
    failed_courses = []
    
    # å®šç¾©å¯èƒ½çš„åˆ—åï¼Œä¸¦æ¨™æº–åŒ–ä»¥ä¾¿åŒ¹é…
    col_mapping = {
        'å­¸å¹´åº¦': ['å­¸å¹´åº¦', 'å­¸å¹´', 'å¹´åº¦'],
        'å­¸æœŸ': ['å­¸æœŸ'],
        'é¸èª²ä»£è™Ÿ': ['é¸èª²ä»£è™Ÿ', 'é¸èª²ä»£è™Ÿ'],
        'ç§‘ç›®åç¨±': ['ç§‘ç›®åç¨±', 'ç§‘ç›®'],
        'å­¸åˆ†': ['å­¸åˆ†'],
        'GPA': ['GPA', 'æˆç¸¾']
    }

    # æ‰¾åˆ°å¯¦éš›çš„åˆ—å
    actual_cols = {}
    for standard_col, possible_names in col_mapping.items():
        # å°å¯èƒ½çš„åç¨±é€²è¡Œæ›´éˆæ´»çš„åŒ¹é… (ä¾‹å¦‚ "å­¸ å¹´ åº¦" ä¹Ÿèƒ½åŒ¹é… "å­¸å¹´åº¦")
        for name in possible_names:
            matching_cols = [col for col in df.columns if normalize_text(name).replace(' ', '') in normalize_text(col).replace(' ', '')]
            if matching_cols:
                actual_cols[standard_col] = matching_cols[0]
                break
        if standard_col not in actual_cols:
            st.warning(f"æœªèƒ½è­˜åˆ¥å‡ºé—œéµæ¬„ä½ï¼š{standard_col}ã€‚è«‹æª¢æŸ¥PDFä¸­çš„è¡¨æ ¼æ¨™é¡Œã€‚")
            return [], [] # å¦‚æœé—œéµæ¬„ä½ç¼ºå¤±ï¼Œå‰‡è¿”å›ç©ºåˆ—è¡¨

    for index, row in df.iterrows():
        try:
            # è·³éå¯èƒ½ä½œç‚ºè¡¨é ­çš„è¡Œï¼ˆä¾‹å¦‚åŒ…å«"å­¸å¹´åº¦"çš„è¡Œï¼Œé€™åœ¨ img2table è™•ç†å¾Œå¯èƒ½ä»ç„¶å­˜åœ¨ï¼‰
            # é€™è£¡çš„åˆ¤æ–·å¯èƒ½éœ€è¦æ›´ç²¾ç¢ºï¼Œå¦‚æœç¬¬ä¸€è¡Œå·²ç¶“è¢«ç§»é™¤ï¼Œå‰‡ä¸éœ€è¦
            # ä½†ç‚ºäº†å¥å£¯æ€§ï¼Œå¯ä»¥ä¿ç•™
            if any(key_word in normalize_text(row.to_string()) for key_word in ['å­¸å¹´åº¦', 'å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA', 'æˆç¸¾']):
                # åˆ¤æ–·æ˜¯å¦ç‚ºå¯¦éš›æ•¸æ“šè¡Œï¼Œé€™è£¡å¯èƒ½éœ€è¦æ›´ç²¾ç¢ºçš„é‚è¼¯
                # ç°¡å–®åˆ¤æ–·ï¼šå¦‚æœæŸè¡Œåœ¨å­¸å¹´åº¦æˆ–å­¸åˆ†ä½ç½®æ˜¯ç©ºçš„ï¼Œå¯èƒ½æ˜¯ç„¡æ•ˆè¡Œ
                if not normalize_text(row[actual_cols['å­¸å¹´åº¦']]).strip() or \
                   not normalize_text(row[actual_cols['å­¸åˆ†']]).strip():
                    continue

            year_term = f"{normalize_text(row[actual_cols['å­¸å¹´åº¦']])}{normalize_text(row[actual_cols['å­¸æœŸ']])}"
            
            credit_str = normalize_text(row[actual_cols['å­¸åˆ†']])
            gpa_str = normalize_text(row[actual_cols['GPA']])

            if not credit_str.strip() or not credit_str.replace('.', '', 1).isdigit():
                continue
            credit = float(credit_str)

            course_data = {
                'å­¸å¹´åº¦å­¸æœŸ': year_term,
                'é¸èª²ä»£è™Ÿ': normalize_text(row[actual_cols['é¸èª²ä»£è™Ÿ']]),
                'ç§‘ç›®åç¨±': normalize_text(row[actual_cols['ç§‘ç›®åç¨±']]),
                'å­¸åˆ†': credit,
                'GPA': gpa_str
            }

            is_pe_class = "é«”è‚²" in course_data['ç§‘ç›®åç¨±']
            is_general_class = "é€šè­˜" in course_data['ç§‘ç›®åç¨±'] or \
                                "äººæ–‡" in course_data['ç§‘ç›®åç¨±'] or \
                                "ç¤¾æœƒ" in course_data['ç§‘ç›®åç¨±'] or \
                                "è‡ªç„¶" in course_data['ç§‘ç›®åç¨±'] or \
                                "æ­·å²" in course_data['ç§‘ç›®åç¨±'] or \
                                "å“²å­¸" in course_data['ç§‘ç›®åç¨±']

            gpa_upper = gpa_str.upper().strip()

            if gpa_upper in ["D", "E", "F", "ä¸è¨ˆ", "æœªé€šé"] or (gpa_upper.isdigit() and float(gpa_upper) < 60):
                failed_courses.append(course_data)
            elif gpa_upper == "é€šé":
                parsed_courses.append(course_data)
            else:
                parsed_courses.append(course_data)

        except KeyError as ke:
            st.warning(f"ç¼ºå°‘é—œéµåˆ—ï¼š{ke}ã€‚è«‹æª¢æŸ¥æå–çš„è¡¨æ ¼æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦çš„æ¨™é¡Œã€‚")
            return [], [] # å¦‚æœåˆ—ååŒ¹é…å¤±æ•—ï¼Œç›´æ¥è¿”å›ç©º
        except Exception as e:
            #st.warning(f"è§£æè¡Œæ™‚å‡ºéŒ¯ï¼š{row.to_dict()} - {e}")
            continue
            
    return parsed_courses, failed_courses


# --- Streamlit æ‡‰ç”¨ç¨‹å¼ç•Œé¢ ---
st.set_page_config(layout="wide", page_title="å­¸åˆ†è¨ˆç®—å™¨", page_icon="ğŸ“")

st.title("ğŸ“š æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å™¨")

st.markdown("""
é€™å€‹å·¥å…·å¯ä»¥å¹«åŠ©æ‚¨åˆ†ææ±æµ·å¤§å­¸çš„æ­·å¹´æˆç¸¾å–® PDF æª”æ¡ˆï¼Œ
è‡ªå‹•è¨ˆç®—æ‚¨å·²ç²å¾—çš„ç¸½å­¸åˆ†ï¼Œä¸¦åˆ—å‡ºé€šéå’Œä¸åŠæ ¼çš„ç§‘ç›®ã€‚
""")

uploaded_file = st.file_uploader("ä¸Šå‚³æ‚¨çš„æˆç¸¾å–® PDF æª”æ¡ˆ", type=["pdf"])

if uploaded_file is not None:
    st.success("æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼")

    with st.spinner("æ­£åœ¨è™•ç† PDFï¼Œé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“..."):
        extracted_df = process_pdf_file(uploaded_file)

    if not extracted_df.empty:
        st.subheader("ğŸ“ æå–åˆ°çš„è¡¨æ ¼æ•¸æ“šé è¦½ï¼š")
        st.dataframe(extracted_df, use_container_width=True)

        if not extracted_df.empty:
            st.subheader("ğŸ“Š å­¸åˆ†è¨ˆç®—çµæœï¼š")
            calculated_courses, failed_courses = parse_course_data(extracted_df)

            total_credits_passed = sum(course['å­¸åˆ†'] for course in calculated_courses)
            
            st.metric(label="âœ… å·²ç²å¾—ç¸½å­¸åˆ† (ä¸å«é«”è‚²èˆ‡æœå‹™å­¸ç¿’)", value=f"{total_credits_passed:.1f}")

            if calculated_courses:
                st.subheader("é€šéçš„ç§‘ç›®åˆ—è¡¨ (è¨ˆå…¥å­¸åˆ†)ï¼š")
                display_passed_df = pd.DataFrame([c for c in calculated_courses if c['å­¸åˆ†'] > 0])
                
                display_cols_passed = ['å­¸å¹´åº¦å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
                final_display_passed_cols = [col for col in display_cols_passed if col in display_passed_df.columns]

                st.dataframe(display_passed_df[final_display_passed_cols], height=300, use_container_width=True)
            else:
                st.info("æ²’æœ‰æ‰¾åˆ°é€šéçš„ç§‘ç›®ã€‚")

            if failed_courses:
                st.subheader("ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ (ä¸è¨ˆå…¥ç¸½å­¸åˆ†)ï¼š")
                failed_df = pd.DataFrame(failed_courses)
                
                display_failed_cols = ['å­¸å¹´åº¦å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
                final_display_failed_cols = [col for col in display_failed_cols if col in failed_df.columns]
                st.dataframe(failed_df[final_display_failed_cols], height=200, use_container_width=True)
                st.info("é€™äº›ç§‘ç›®å› æˆç¸¾ä¸åŠæ ¼ ('D', 'E', 'F' ç­‰) è€Œæœªè¨ˆå…¥ç¸½å­¸åˆ†ã€‚")

            if calculated_courses or failed_courses:
                if calculated_courses:
                    csv_data_passed = pd.DataFrame(calculated_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰é€šéçš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_passed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_calculated_courses.csv",
                        mime="text/csv",
                        key="download_passed_btn"
                    )
                if failed_courses:
                    csv_data_failed = pd.DataFrame(failed_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_failed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_failed_courses.csv",
                        mime="text/csv",
                        key="download_failed_btn"
                    )
            
        else:
            st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦å…¶ä»–æ–‡ä»¶ã€‚")
    else:
        st.error("ç„¡æ³•å¾ PDF æª”æ¡ˆä¸­æå–ä»»ä½•æœ‰æ•ˆæ•¸æ“šã€‚è«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼æˆ–å…§å®¹ã€‚")

st.markdown("---")
st.markdown("é–‹ç™¼è€…ï¼š[æ‚¨çš„åå­—æˆ–è¯çµ¡æ–¹å¼ (é¸å¡«)]")
