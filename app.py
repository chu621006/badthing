import streamlit as st
import pandas as pd
import pdfplumber
import collections
import re
import io

# å°å…¥ img2table çš„ PDF é¡ å’Œ TesseractOCR
from img2table.document import PDF as Img2TablePDF
from img2table.ocr import TesseractOCR

# --- è¼”åŠ©å‡½æ•¸ ---
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾ pdfplumber æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    å°‡å¤šå€‹ç©ºç™½å­—å…ƒï¼ˆåŒ…æ‹¬æ›è¡Œï¼‰æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å…©ç«¯ç©ºç™½ã€‚
    """
    if cell_content is None:
        return ""

    text = ""
    # æª¢æŸ¥æ˜¯å¦æ˜¯ pdfplumber çš„ Text ç‰©ä»¶ (å®ƒæœƒæœ‰ .text å±¬æ€§)
    if hasattr(cell_content, 'text'):
        text = str(cell_content.text)
    # å¦‚æœä¸æ˜¯ Text ç‰©ä»¶ï¼Œä½†æœ¬èº«æ˜¯å­—ä¸²
    elif isinstance(cell_content, str):
        text = cell_content
    # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦è½‰æ›ç‚ºå­—ä¸²
    else:
        text = str(cell_content)
    
    return re.sub(r'\s+', ' ', text).strip()

def make_unique_columns(columns_list):
    """
    å°‡åˆ—è¡¨ä¸­çš„æ¬„ä½åç¨±è½‰æ›ç‚ºå”¯ä¸€çš„åç¨±ï¼Œè™•ç†é‡è¤‡å’Œç©ºå­—ä¸²ã€‚
    å¦‚æœé‡åˆ°é‡è¤‡æˆ–ç©ºå­—ä¸²ï¼Œæœƒæ·»åŠ å¾Œç¶´ (ä¾‹å¦‚ 'Column_1', 'æ¬„ä½_2')ã€‚
    """
    seen = collections.defaultdict(int)
    unique_columns = []
    for col in columns_list:
        original_col = col
        if not col or col.isspace(): # å¦‚æœæ¬„ä½æ˜¯ç©ºå­—ä¸²æˆ–åªæœ‰ç©ºæ ¼
            col = "Unnamed_Column"
        
        if seen[col] > 0:
            unique_col = f"{col}_{seen[col]}"
            while unique_col in unique_columns: # é¿å…ç”Ÿæˆå†æ¬¡é‡è¤‡çš„åç¨±
                seen[col] += 1
                unique_col = f"{col}_{seen[col]}"
            unique_columns.append(unique_col)
        else:
            unique_columns.append(col)
        seen[original_col] += 1 # é€™è£¡ä½¿ç”¨åŸå§‹åç¨±ä¾†è¨ˆæ•¸ï¼Œç¢ºä¿åŸåç¨±åœ¨å”¯ä¸€åŒ–å‰ä¹Ÿèƒ½è¢«è¨ˆæ•¸
        
    return unique_columns

def is_gpa_valid(gpa_str):
    """æª¢æŸ¥ GPA æ¬„ä½æ˜¯å¦ç‚ºæœ‰æ•ˆå€¼ (å­—æ¯æˆ–æ•¸å­—ï¼Œæ’é™¤ 'é€šé' ç­‰éåˆ†æ•¸é¡å‹)"""
    if not isinstance(gpa_str, str):
        return False
    # éæ¿¾æ‰ 'é€šé', 'æŠµå…', 'å¿…ä¿®', 'é¸ä¿®' ç­‰éå¯¦éš›GPAæˆç¸¾
    return bool(re.match(r'^[A-Ea-eDFdfC]{1}[+-]?$|^[0-9.]+$', gpa_str)) and gpa_str not in ["é€šé", "æŠµå…", "å¿…ä¿®", "é¸ä¿®", "é€šé"]

# --- ä¸»å‡½æ•¸ï¼šè™•ç† PDF æª”æ¡ˆ ---
def process_pdf_file(uploaded_file):
    all_grades_data = []
    
    # åˆ¤æ–· PDF æ˜¯å¦ç‚ºåœ–ç‰‡å‹
    st.info("æ­£åœ¨å˜—è©¦ä½¿ç”¨ pdfplumber æå–æ–‡æœ¬å’Œè¡¨æ ¼...")
    is_image_pdf = False
    try:
        with pdfplumber.open(uploaded_file) as pdf:
            if not any(page.extract_text() for page in pdf.pages):
                is_image_pdf = True
    except Exception as e:
        st.warning(f"pdfplumber è™•ç†éŒ¯èª¤ï¼Œå¯èƒ½æ˜¯åœ–ç‰‡å‹ PDFï¼š{e}")
        is_image_pdf = True # å¦‚æœ pdfplumber å¤±æ•—ï¼Œå‰‡è¦–ç‚ºåœ–ç‰‡å‹ PDF

    if is_image_pdf:
        st.info("æª¢æ¸¬åˆ°åœ–ç‰‡å‹ PDF æˆ–éæ¨™æº– PDF æ ¼å¼ï¼Œå˜—è©¦ä½¿ç”¨ img2table é€²è¡Œ OCR è™•ç†...")
        return process_image_pdf_with_ocr(uploaded_file)
    else:
        st.info("æª¢æ¸¬åˆ°å¯é¸å–æ–‡å­—çš„ PDFï¼Œä½¿ç”¨ pdfplumber æå–è¡¨æ ¼ã€‚")
        return process_text_pdf_with_pdfplumber(uploaded_file)

def process_text_pdf_with_pdfplumber(uploaded_file):
    all_grades_data_dfs = []
    try:
        with pdfplumber.open(uploaded_file) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                for table_idx, table in enumerate(tables):
                    header = [normalize_text(cell) for cell in table[0]] if table else []
                    
                    # å˜—è©¦æ¸…ç†æˆ–æ˜ å°„æ¨™é ­
                    cleaned_header = []
                    for h in header:
                        if "å­¸å¹´" in h and "åº¦" in h:
                            cleaned_header.append("å­¸å¹´åº¦")
                        elif "å­¸æœŸ" in h:
                            cleaned_header.append("å­¸æœŸ")
                        elif "ç§‘ç›®åç¨±" in h:
                            cleaned_header.append("ç§‘ç›®åç¨±")
                        elif "å­¸åˆ†" in h:
                            cleaned_header.append("å­¸åˆ†")
                        elif "GPA" in h:
                            cleaned_header.append("GPA")
                        else:
                            cleaned_header.append(h)
                    
                    unique_header = make_unique_columns(cleaned_header)

                    if not unique_header:
                        continue

                    data = []
                    for row in table[1:]: # å¾ç¬¬äºŒè¡Œé–‹å§‹æ˜¯æ•¸æ“š
                        row_data = [normalize_text(cell) for cell in row]
                        # ç¢ºä¿è¡Œæ•¸æ“šé•·åº¦èˆ‡æ¨™é ­åŒ¹é…ï¼Œä¸è¶³è£œç©ºå­—ä¸²
                        if len(row_data) < len(unique_header):
                            row_data.extend([''] * (len(unique_header) - len(row_data)))
                        # å¦‚æœè¡Œæ•¸æ“šé•·åº¦è¶…éæ¨™é ­ï¼Œå‰‡æˆªæ–·
                        elif len(row_data) > len(unique_header):
                            row_data = row_data[:len(unique_header)]
                        data.append(row_data)
                    
                    if data:
                        df = pd.DataFrame(data, columns=unique_header)
                        all_grades_data_dfs.append(df)
        
        return pd.concat(all_grades_data_dfs, ignore_index=True) if all_grades_data_dfs else pd.DataFrame()

    except Exception as e:
        st.error(f"ä½¿ç”¨ pdfplumber æå–è¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return pd.DataFrame()

def process_image_pdf_with_ocr(uploaded_file):
    all_grades_data_dfs = []
    
    try:
        # ä½¿ç”¨ img2table è™•ç†åœ–ç‰‡å‹ PDF
        # æ³¨æ„ï¼šé€™è£¡çš„ TesseractOCR æœƒè‡ªå‹•å°‹æ‰¾ TESSDATA_PREFIX ç’°å¢ƒè®Šæ•¸
        ocr = TesseractOCR(lang="chi_tra") # æŒ‡å®šç¹é«”ä¸­æ–‡èªè¨€åŒ…
        
        # å°‡ uploaded_file è½‰æ›ç‚º BytesIO ç‰©ä»¶ï¼Œä¾› img2table ä½¿ç”¨
        pdf_bytes = io.BytesIO(uploaded_file.getvalue())
        
        # å‰µå»º Img2TablePDF ç‰©ä»¶
        doc = Img2TablePDF(src=pdf_bytes)
        
        # å¾æ–‡æª”ä¸­æå–è¡¨æ ¼
        extracted_tables = doc.extract_tables(ocr=ocr, implicit_lines=False, borderless_tables=False)

        if not extracted_tables:
            st.warning("img2table æœªèƒ½å¾åœ–ç‰‡å‹ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚")
            return pd.DataFrame()

        for page_idx, tables_on_page in extracted_tables.items():
            for table_idx, table in enumerate(tables_on_page):
                # img2table è¿”å›çš„ table.df å·²ç¶“æ˜¯ DataFrame
                df = table.df
                
                # å°æ–¼ img2table æå–çš„ DataFrameï¼Œä¹Ÿé€²è¡Œåˆ—åæ¨™æº–åŒ–å’Œæ•¸æ“šæ¸…ç†
                if not df.empty:
                    # å¦‚æœ img2table å°å‡ºçš„ DataFrame æœ‰é»˜èªæ•¸å­—åˆ—åï¼Œå˜—è©¦æŸ¥æ‰¾ä¸¦é‡å‘½å
                    # å¦å‰‡ï¼Œä¿æŒ img2table åµæ¸¬åˆ°çš„åˆ—å
                    header = [normalize_text(col) for col in df.columns]
                    
                    # å˜—è©¦æ˜ å°„å¸¸è¦‹çš„æˆç¸¾å–®åˆ—å
                    cleaned_header = []
                    for h in header:
                        if "å­¸å¹´" in h and "åº¦" in h:
                            cleaned_header.append("å­¸å¹´åº¦")
                        elif "å­¸æœŸ" in h:
                            cleaned_header.append("å­¸æœŸ")
                        elif "ç§‘ç›®åç¨±" in h:
                            cleaned_header.append("ç§‘ç›®åç¨±")
                        elif "å­¸åˆ†" in h:
                            cleaned_header.append("å­¸åˆ†")
                        elif "GPA" in h:
                            cleaned_header.append("GPA")
                        else:
                            cleaned_header.append(h)
                    
                    df.columns = make_unique_columns(cleaned_header)
                    
                    # å°æ¯å€‹å–®å…ƒæ ¼çš„æ•¸æ“šé€²è¡Œæ¨™æº–åŒ–
                    df = df.applymap(normalize_text)
                    all_grades_data_dfs.append(df)

        return pd.concat(all_grades_data_dfs, ignore_index=True) if all_grades_data_dfs else pd.DataFrame()

    except Exception as e:
        st.error(f"ä½¿ç”¨ img2table è™•ç†åœ–ç‰‡ PDF æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.info("è«‹ç¢ºä¿ Dockerfile æ­£ç¢ºå®‰è£äº† Tesseract OCR å¼•æ“å’Œæ‰€æœ‰ç›¸é—œçš„ç³»çµ±ä¾è³´ï¼ˆå¦‚ libGL.so.1 ç­‰ï¼‰ã€‚")
        return pd.DataFrame()


# --- Streamlit æ‡‰ç”¨ä»‹é¢ ---
st.set_page_config(layout="wide")
st.title("æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å™¨ ğŸ“")
st.markdown("---")

uploaded_file = st.file_uploader("è«‹ä¸Šå‚³æ‚¨çš„ PDF æˆç¸¾å–®æª”æ¡ˆ", type="pdf")

if uploaded_file is not None:
    st.success("æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼")

    with st.spinner("æ­£åœ¨è™•ç† PDFï¼Œé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“..."):
        # æ ¹æ“š PDF é¡å‹é¸æ“‡è™•ç†æ–¹å¼
        extracted_df = process_pdf_file(uploaded_file)

    if not extracted_df.empty:
        st.subheader("æå–åˆ°çš„åŸå§‹è¡¨æ ¼æ•¸æ“š:")
        st.dataframe(extracted_df, height=300, use_container_width=True)

        st.subheader("æ•¸æ“šæ¸…æ´—èˆ‡å­¸åˆ†è¨ˆç®—çµæœ:")
        
        # å˜—è©¦è­˜åˆ¥é—œéµæ¬„ä½ï¼Œå„ªå…ˆä½¿ç”¨æ¨™æº–åç¨±
        col_mapping = {
            'å­¸å¹´åº¦': ['å­¸å¹´åº¦', 'å­¸å¹´', 'Academic Year'],
            'å­¸æœŸ': ['å­¸æœŸ', 'æœŸ', 'Semester'],
            'ç§‘ç›®åç¨±': ['ç§‘ç›®åç¨±', 'ç§‘ç›®', 'Course Name'],
            'å­¸åˆ†': ['å­¸åˆ†', 'Credit'],
            'GPA': ['GPA', 'æˆç¸¾', 'Grade']
        }

        found_cols = {}
        for standard_col, possible_names in col_mapping.items():
            for name in possible_names:
                if name in extracted_df.columns:
                    found_cols[standard_col] = name
                    break
            
            # å¦‚æœæ¨™æº–åˆ—åæ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦å¾ Make_unique_columns ä¸­æ‰¾å¸¶å¾Œç¶´çš„
            if standard_col not in found_cols:
                for col_name_in_df in extracted_df.columns:
                    if col_name_in_df.startswith(standard_col) and (standard_col not in found_cols):
                         found_cols[standard_col] = col_name_in_df
                         break
        
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½æ‰¾åˆ°
        required_cols = ["å­¸å¹´åº¦", "å­¸æœŸ", "ç§‘ç›®åç¨±", "å­¸åˆ†", "GPA"]
        
        if not all(col in found_cols for col in required_cols):
            st.warning("æœªèƒ½è‡ªå‹•è­˜åˆ¥æ‰€æœ‰å¿…è¦çš„æˆç¸¾å–®æ¬„ä½ (å­¸å¹´åº¦, å­¸æœŸ, ç§‘ç›®åç¨±, å­¸åˆ†, GPA)ã€‚")
            st.info("è«‹æª¢æŸ¥ä¸Šå‚³çš„ PDF æ ¼å¼ï¼Œæˆ–æ‰‹å‹•æŒ‡å®šæ¬„ä½åç¨±ã€‚")
            
            # è®“ç”¨æˆ¶æ‰‹å‹•é¸æ“‡æ¬„ä½
            st.subheader("æ‰‹å‹•æŒ‡å®šæ¬„ä½ï¼š")
            selected_year_col = st.selectbox("è«‹é¸æ“‡ 'å­¸å¹´åº¦' æ¬„ä½ï¼š", [''] + list(extracted_df.columns))
            selected_semester_col = st.selectbox("è«‹é¸æ“‡ 'å­¸æœŸ' æ¬„ä½ï¼š", [''] + list(extracted_df.columns))
            selected_course_name_col = st.selectbox("è«‹é¸æ“‡ 'ç§‘ç›®åç¨±' æ¬„ä½ï¼š", [''] + list(extracted_df.columns))
            selected_credit_col = st.selectbox("è«‹é¸æ“‡ 'å­¸åˆ†' æ¬„ä½ï¼š", [''] + list(extracted_df.columns))
            selected_gpa_col = st.selectbox("è«‹é¸æ“‡ 'GPA' / 'æˆç¸¾' æ¬„ä½ï¼š", [''] + list(extracted_df.columns))

            if st.button("ç¢ºèªæ‰‹å‹•é¸æ“‡"):
                # æ›´æ–° found_cols
                if selected_year_col: found_cols["å­¸å¹´åº¦"] = selected_year_col
                if selected_semester_col: found_cols["å­¸æœŸ"] = selected_semester_col
                if selected_course_name_col: found_cols["ç§‘ç›®åç¨±"] = selected_course_name_col
                if selected_credit_col: found_cols["å­¸åˆ†"] = selected_credit_col
                if selected_gpa_col: found_cols["GPA"] = selected_gpa_col
                st.experimental_rerun() # é‡æ–°é‹è¡Œä»¥æ‡‰ç”¨é¸æ“‡

            if not all(col in found_cols for col in required_cols):
                st.error("ä»ç„¶ç¼ºå°‘å¿…è¦çš„æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œå­¸åˆ†è¨ˆç®—ã€‚è«‹ç¢ºä¿æ‰€æœ‰é—œéµæ¬„ä½éƒ½å·²æ­£ç¢ºæŒ‡å®šã€‚")
                st.stop() # åœæ­¢åŸ·è¡Œï¼Œç›´åˆ°æ¬„ä½è¢«æ­£ç¢ºé¸å®š
        
        # é‡æ–°å‘½å DataFrame æ¬„ä½ä»¥æ¨™æº–åŒ–
        renamed_df = extracted_df.rename(columns={v: k for k, v in found_cols.items()})

        # éæ¿¾æ‰åŒ…å«éæˆç¸¾æ•¸æ“šçš„è¡Œ (å¦‚åªæœ‰æ¨™é¡Œçš„è¡Œ)
        # ç¯©é¸æ¢ä»¶ï¼š'å­¸åˆ†' å¿…é ˆæ˜¯æ•¸å­—ï¼Œ'GPA' å¿…é ˆæ˜¯æœ‰æ•ˆçš„æˆç¸¾æ ¼å¼
        initial_filtered_df = renamed_df[
            pd.to_numeric(renamed_df['å­¸åˆ†'], errors='coerce').notna() &
            renamed_df['GPA'].apply(is_gpa_valid)
        ].copy() # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning

        # ç¢ºä¿å­¸åˆ†æ˜¯æ•¸å€¼é¡å‹
        initial_filtered_df['å­¸åˆ†'] = pd.to_numeric(initial_filtered_df['å­¸åˆ†'], errors='coerce')

        # éæ¿¾æ‰å­¸åˆ†ç‚º0çš„ç§‘ç›® (é€šå¸¸æ˜¯é«”è‚²æˆ–è»è¨“)
        filtered_df = initial_filtered_df[initial_filtered_df['å­¸åˆ†'] > 0].copy()

        # éæ¿¾ä¸åŠæ ¼ç§‘ç›® (é€šå¸¸ç‚º D, E, F ç­‰ï¼Œæˆ–ç‰¹å®šçš„ä½æ–¼60çš„æ•¸å­—)
        passed_grades_df = filtered_df[~filtered_df['GPA'].isin(['D', 'd', 'E', 'e', 'F', 'f'])].copy()

        # æå–ä¸åŠæ ¼çš„ç§‘ç›®
        failed_grades_df = filtered_df[filtered_df['GPA'].isin(['D', 'd', 'E', 'e', 'F', 'f'])].copy()

        # è¨ˆç®—é€šéçš„ç¸½å­¸åˆ†
        total_credits = passed_grades_df['å­¸åˆ†'].sum()
        st.success(f"âœ”ï¸ é€šéä¸”è¨ˆå…¥ç¸½å­¸åˆ†çš„ç¸½å­¸åˆ†ï¼š**{total_credits}** å­¸åˆ†")

        # é¡¯ç¤ºé€šéçš„ç§‘ç›®åˆ—è¡¨
        st.subheader("é€šéä¸”è¨ˆå…¥ç¸½å­¸åˆ†çš„ç§‘ç›®åˆ—è¡¨ï¼š")
        if not passed_grades_df.empty:
            display_cols = ['å­¸å¹´åº¦', 'å­¸æœŸ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
            final_display_cols = [col for col in display_cols if col in passed_grades_df.columns]
            st.dataframe(passed_grades_df[final_display_cols], height=300, use_container_width=True)
            st.info("é€™äº›ç§‘ç›®å·²è¨ˆå…¥ç¸½å­¸åˆ†ã€‚")
            calculated_courses = passed_grades_df[final_display_cols].to_dict('records')
        else:
            st.warning("æ²’æœ‰æ‰¾åˆ°é€šéä¸”è¨ˆå…¥ç¸½å­¸åˆ†çš„ç§‘ç›®ã€‚")
            calculated_courses = []

        # é¡¯ç¤ºä¸åŠæ ¼ç§‘ç›®åˆ—è¡¨ (å¦‚æœæœ‰)
        st.subheader("æœªè¨ˆå…¥ç¸½å­¸åˆ†çš„ä¸åŠæ ¼ç§‘ç›®åˆ—è¡¨ï¼š")
        if not failed_grades_df.empty:
            display_failed_cols = ['å­¸å¹´åº¦', 'å­¸æœŸ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
            final_display_failed_cols = [col for col in display_failed_cols if col in failed_grades_df.columns]
            st.dataframe(failed_grades_df[final_display_failed_cols], height=200, use_container_width=True)
            st.info("é€™äº›ç§‘ç›®å› æˆç¸¾ä¸åŠæ ¼ ('D', 'E', 'F' ç­‰) è€Œæœªè¨ˆå…¥ç¸½å­¸åˆ†ã€‚") # æ›´æ–°è¨Šæ¯
            failed_courses = failed_grades_df[final_display_failed_cols].to_dict('records')
        else:
            st.info("æ²’æœ‰æ‰¾åˆ°ä¸åŠæ ¼çš„ç§‘ç›®ã€‚")
            failed_courses = []

        # æä¾›ä¸‹è¼‰é¸é … 
        if calculated_courses or failed_courses:
            if calculated_courses:
                csv_data_passed = pd.DataFrame(calculated_courses).to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ä¸‹è¼‰é€šéçš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                    data=csv_data_passed,
                    file_name=f"{uploaded_file.name.replace('.pdf', '')}_calculated_courses.csv",
                    mime="text/csv",
                    key="download_passed_btn"
                )
            if failed_courses:
                csv_data_failed = pd.DataFrame(failed_courses).to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ä¸‹è¼‰ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                    data=csv_data_failed,
                    file_name=f"{uploaded_file.name.replace('.pdf', '')}_failed_courses.csv",
                    mime="text/csv",
                    key="download_failed_btn"
                )
        
    else:
        st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦å…¶ä»–æ ¼å¼ã€‚")