import streamlit as st
import pandas as pd
import pdfplumber
import collections
import re
import io

from PIL import Image # å°å…¥ Pillow å‡½å¼åº«
import pytesseract # å°å…¥ pytesseract

# **æ–°å¢é€™ä¸€è¡Œï¼Œæ˜ç¢ºæŒ‡å®š tesseract åŸ·è¡Œæª”çš„è·¯å¾‘**
# åœ¨åŸºæ–¼ Debian/Ubuntu çš„ Docker æ˜ åƒä¸­ï¼Œtesseract åŸ·è¡Œæª”é€šå¸¸æœƒå®‰è£åœ¨ /usr/bin/tesseract
pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'

# å°å…¥ img2table çš„ PDF é¡ å’Œ TesseractOCR
from img2table.document import PDF as Img2TablePDF
from img2table.ocr import TesseractOCR

# --- è¼”åŠ©å‡½æ•¸ ---
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾ pdfplumber æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    å°‡å¤šå€‹ç©ºç™½å­—å…ƒï¼ˆåŒ…æ‹¬æ›è¡Œï¼‰æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å…©ç«¯ç©ºç™½ã€‚
    """
    if cell_content is None:
        return ""

    text = ""
    # æª¢æŸ¥æ˜¯å¦æ˜¯ pdfplumber çš„ Text ç‰©ä»¶ (å®ƒæœƒæœ‰ .text å±¬æ€§)
    if hasattr(cell_content, 'text'):
        text = str(cell_content.text)
    # å¦‚æœä¸æ˜¯ Text ç‰©ä»¶ï¼Œä½†æœ¬èº«æ˜¯å­—ä¸²
    elif isinstance(cell_content, str):
        text = cell_content
    # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦è½‰æ›ç‚ºå­—ä¸²
    else:
        text = str(cell_content)
    
    return re.sub(r'\s+', ' ', text).strip()

def make_unique_columns(columns_list):
    """
    å°‡åˆ—è¡¨ä¸­çš„æ¬„ä½åç¨±è½‰æ›ç‚ºå”¯ä¸€çš„åç¨±ï¼Œè™•ç†é‡è¤‡å’Œç©ºå­—ä¸²ã€‚
    å¦‚æœé‡åˆ°é‡è¤‡æˆ–ç©ºå­—ä¸²ï¼Œæœƒæ·»åŠ å¾Œç¶´ (ä¾‹å¦‚ 'Column_1', 'æ¬„ä½_2')ã€‚
    """
    seen = collections.defaultdict(int)
    unique_columns = []
    for col in columns_list:
        original_col = col
        if not col.strip():  # å¦‚æœæ¬„ä½æ˜¯ç©ºå­—ä¸²ï¼Œçµ¦ä¸€å€‹é è¨­åç¨±
            col = "ç©ºç™½æ¬„ä½"
        
        # æª¢æŸ¥æ˜¯å¦é‡è¤‡
        if col in seen:
            seen[col] += 1
            unique_columns.append(f"{col}_{seen[col]}")
        else:
            seen[col] = 0
            unique_columns.append(col)
    return unique_columns

def process_pdf_file(uploaded_file):
    """
    è™•ç† PDF æª”æ¡ˆï¼Œæå–è¡¨æ ¼æ•¸æ“šï¼Œä¸¦å°‡å…¶è½‰æ›ç‚º DataFrameã€‚
    å„ªå…ˆä½¿ç”¨ pdfplumber æå–ï¼Œè‹¥å¤±æ•—å‰‡å˜—è©¦ img2table + OCRã€‚
    """
    st.info("æ­£åœ¨å˜—è©¦ä½¿ç”¨ pdfplumber æå–è¡¨æ ¼...")
    try:
        with pdfplumber.open(uploaded_file) as pdf:
            all_tables = []
            for page in pdf.pages:
                tables = page.extract_tables()
                for table in tables:
                    # å°‡æ¯å€‹è¡¨æ ¼è½‰æ›ç‚º DataFrame
                    df = pd.DataFrame(table)
                    all_tables.append(df)

            if all_tables:
                # åˆä½µæ‰€æœ‰æå–åˆ°çš„è¡¨æ ¼
                merged_df = pd.concat(all_tables, ignore_index=True)
                # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œç‚ºæ–°çš„åˆ—åï¼Œä¸¦è™•ç†é‡è¤‡å’Œç©ºåˆ—å
                new_columns = merged_df.iloc[0].apply(normalize_text).tolist()
                new_columns = make_unique_columns(new_columns)
                merged_df.columns = new_columns
                # åˆªé™¤ç¬¬ä¸€è¡Œï¼ˆåŸåˆ—åè¡Œï¼‰
                merged_df = merged_df[1:].reset_index(drop=True)
                # å°æ‰€æœ‰å–®å…ƒæ ¼å…§å®¹é€²è¡Œæ¨™æº–åŒ–
                merged_df = merged_df.applymap(normalize_text)
                st.success("æˆåŠŸä½¿ç”¨ pdfplumber æå–è¡¨æ ¼ã€‚")
                return merged_df
            else:
                st.warning("pdfplumber æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼ã€‚å˜—è©¦ä½¿ç”¨ OCR æ–¹å¼è™•ç†åœ–ç‰‡ PDFã€‚")
                return process_image_pdf_with_ocr(uploaded_file)
    except Exception as e:
        st.warning(f"pdfplumber è™•ç†å¤±æ•—: {e}ã€‚å˜—è©¦ä½¿ç”¨ OCR æ–¹å¼è™•ç†åœ–ç‰‡ PDFã€‚")
        return process_image_pdf_with_ocr(uploaded_file)

def process_image_pdf_with_ocr(uploaded_file):
    """
    ä½¿ç”¨ img2table + Tesseract OCR è™•ç†åœ–ç‰‡ PDFï¼Œæå–è¡¨æ ¼æ•¸æ“šã€‚
    """
    st.info("æ­£åœ¨å˜—è©¦ä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼...")
    try:
        # å°‡ä¸Šå‚³çš„æª”æ¡ˆè½‰æ›ç‚º BytesIO ç‰©ä»¶ï¼Œä¾› img2table ä½¿ç”¨
        pdf_bytes = io.BytesIO(uploaded_file.read())

        # åˆå§‹åŒ– Tesseract OCR
        # lang åƒæ•¸æ‡‰èˆ‡æ‚¨åœ¨ Dockerfile ä¸­ä¸‹è¼‰çš„èªè¨€åŒ…ç›¸åŒ¹é…
        ocr = TesseractOCR(lang="chi_tra") # æŒ‡å®šç¹é«”ä¸­æ–‡èªè¨€åŒ…

        # å‰µå»º Img2TablePDF ç‰©ä»¶
        doc = Img2TablePDF(src=pdf_bytes, pages="all")

        # æå–è¡¨æ ¼
        extracted_tables = doc.extract_tables(ocr=ocr,
                                              implicit_rows=False,
                                              borderless_tables=False,
                                              min_confidence=75) # æé«˜ç½®ä¿¡åº¦ï¼Œæ¸›å°‘éŒ¯èª¤

        if extracted_tables:
            all_dfs = []
            for page_num, tables_on_page in extracted_tables.items():
                for idx, table in enumerate(tables_on_page):
                    df = pd.DataFrame(table.content)
                    all_dfs.append(df)

            if all_dfs:
                merged_df = pd.concat(all_dfs, ignore_index=True)
                # å°æ‰€æœ‰å–®å…ƒæ ¼å…§å®¹é€²è¡Œæ¨™æº–åŒ–
                merged_df = merged_df.applymap(normalize_text)
                st.success("æˆåŠŸä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼ã€‚")
                return merged_df
            else:
                st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚")
                return pd.DataFrame() # è¿”å›ç©º DataFrame
        else:
            st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚")
            return pd.DataFrame() # è¿”å›ç©º DataFrame

    except Exception as e:
        st.error(f"ä½¿ç”¨ img2table è™•ç†åœ–ç‰‡ PDF æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹ç¢ºä¿ Dockerfile æ­£ç¢ºå®‰è£äº† Tesseract OCR å¼•æ“å’Œæ‰€æœ‰ç›¸é—œçš„ç³»çµ±ä¾è³´ï¼ˆå¦‚ libGL.so.1 ç­‰ï¼‰ã€‚")
        return pd.DataFrame() # è¿”å›ç©º DataFrame

def parse_course_data(df):
    """
    å¾ DataFrame ä¸­è§£æèª²ç¨‹æ•¸æ“šï¼Œè¨ˆç®—ç¸½å­¸åˆ†å’Œä¸åŠæ ¼å­¸åˆ†ã€‚
    """
    parsed_courses = []
    failed_courses = []
    
    # å®šç¾©å¯èƒ½çš„åˆ—åï¼Œä¸¦æ¨™æº–åŒ–ä»¥ä¾¿åŒ¹é…
    # å°‡ 'å­¸å¹´'ã€'å¹´åº¦' è¦–ç‚º 'å­¸å¹´åº¦'
    # å°‡ 'å­¸æœŸ' è¦–ç‚º 'å­¸æœŸ'
    # å°‡ 'é¸èª²ä»£è™Ÿ'ã€'é¸èª²ä»£è™Ÿ' è¦–ç‚º 'é¸èª²ä»£è™Ÿ'
    # å°‡ 'ç§‘ç›®åç¨±'ã€'ç§‘ç›®'ã€'ç§‘ç›®åç¨±' è¦–ç‚º 'ç§‘ç›®åç¨±'
    # å°‡ 'å­¸åˆ†'ã€'å­¸åˆ†' è¦–ç‚º 'å­¸åˆ†'
    # å°‡ 'GPA'ã€'æˆç¸¾' è¦–ç‚º 'GPA'
    
    col_mapping = {
        'å­¸å¹´åº¦': ['å­¸å¹´åº¦', 'å­¸å¹´', 'å¹´åº¦'],
        'å­¸æœŸ': ['å­¸æœŸ'],
        'é¸èª²ä»£è™Ÿ': ['é¸èª²ä»£è™Ÿ', 'é¸èª²ä»£è™Ÿ'],
        'ç§‘ç›®åç¨±': ['ç§‘ç›®åç¨±', 'ç§‘ç›®'],
        'å­¸åˆ†': ['å­¸åˆ†'],
        'GPA': ['GPA', 'æˆç¸¾']
    }

    # æ‰¾åˆ°å¯¦éš›çš„åˆ—å
    actual_cols = {}
    for standard_col, possible_names in col_mapping.items():
        for name in possible_names:
            # ä½¿ç”¨åˆ—è¡¨æ¨å°å¼ä¾†æª¢æŸ¥åŒ…å«é—œä¿‚
            matching_cols = [col for col in df.columns if name in col]
            if matching_cols:
                # é¸æ“‡ç¬¬ä¸€å€‹åŒ¹é…çš„åˆ—ä½œç‚ºå¯¦éš›åˆ—
                actual_cols[standard_col] = matching_cols[0]
                break
        if standard_col not in actual_cols:
            st.warning(f"æœªèƒ½è­˜åˆ¥å‡ºé—œéµæ¬„ä½ï¼š{standard_col}ã€‚è«‹æª¢æŸ¥PDFä¸­çš„è¡¨æ ¼æ¨™é¡Œã€‚")
            return [], [] # å¦‚æœé—œéµæ¬„ä½ç¼ºå¤±ï¼Œå‰‡è¿”å›ç©ºåˆ—è¡¨

    for index, row in df.iterrows():
        try:
            # è·³éå¯èƒ½ä½œç‚ºè¡¨é ­çš„è¡Œï¼ˆä¾‹å¦‚åŒ…å«"å­¸å¹´åº¦"çš„è¡Œï¼‰
            if any(key_word in normalize_text(row.to_string()) for key_word in ['å­¸å¹´åº¦', 'å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA', 'æˆç¸¾']):
                continue

            # ä½¿ç”¨è­˜åˆ¥åˆ°çš„å¯¦éš›åˆ—åä¾†æå–æ•¸æ“š
            year_term = f"{normalize_text(row[actual_cols['å­¸å¹´åº¦']])}{normalize_text(row[actual_cols['å­¸æœŸ']])}"
            
            # å˜—è©¦å®‰å…¨åœ°è½‰æ›å­¸åˆ†å’Œ GPA
            credit_str = normalize_text(row[actual_cols['å­¸åˆ†']])
            gpa_str = normalize_text(row[actual_cols['GPA']])

            # éæ¿¾æ‰éæ•¸å­—çš„å­¸åˆ†æˆ–GPA
            if not credit_str.strip() or not credit_str.replace('.', '', 1).isdigit():
                continue # è·³éç„¡æ³•è§£æå­¸åˆ†çš„è¡Œ
            credit = float(credit_str)

            course_data = {
                'å­¸å¹´åº¦å­¸æœŸ': year_term,
                'é¸èª²ä»£è™Ÿ': normalize_text(row[actual_cols['é¸èª²ä»£è™Ÿ']]),
                'ç§‘ç›®åç¨±': normalize_text(row[actual_cols['ç§‘ç›®åç¨±']]),
                'å­¸åˆ†': credit,
                'GPA': gpa_str
            }

            # åˆ¤æ–·æ˜¯å¦ç‚ºé«”è‚²èª²æˆ–é€šè­˜èª² (æ ¹æ“šåç¨±åˆ¤æ–·ï¼Œå¯èƒ½éœ€è¦æ›´ç²¾ç¢ºçš„è¦å‰‡)
            is_pe_class = "é«”è‚²" in course_data['ç§‘ç›®åç¨±']
            is_general_class = "é€šè­˜" in course_data['ç§‘ç›®åç¨±'] or \
                                "äººæ–‡" in course_data['ç§‘ç›®åç¨±'] or \
                                "ç¤¾æœƒ" in course_data['ç§‘ç›®åç¨±'] or \
                                "è‡ªç„¶" in course_data['ç§‘ç›®åç¨±'] or \
                                "æ­·å²" in course_data['ç§‘ç›®åç¨±'] or \
                                "å“²å­¸" in course_data['ç§‘ç›®åç¨±']


            # åˆ¤æ–·æ˜¯å¦åŠæ ¼
            # è½‰æ› GPA ç‚ºå¤§å¯«ï¼Œä¸¦ç§»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºç™½ï¼Œä»¥ä¾¿åŒ¹é…
            gpa_upper = gpa_str.upper().strip()

            if gpa_upper in ["D", "E", "F", "ä¸è¨ˆ", "æœªé€šé"] or (gpa_upper.isdigit() and float(gpa_upper) < 60):
                # æ’é™¤é«”è‚²èª²å’Œé€šè­˜èª²çš„ä¸åŠæ ¼æƒ…æ³ï¼Œé™¤éæ˜¯å¿…ä¿®é€šè­˜æˆ–ç‰¹å®šè¦æ±‚
                # é€™è£¡å‡è¨­é«”è‚²å’Œé€šè­˜ä¸åŠæ ¼ä¸å½±éŸ¿ç¸½å­¸åˆ†çµ±è¨ˆï¼Œä½†ä»è¨˜éŒ„åœ¨ä¸åŠæ ¼åˆ—è¡¨ä¸­
                failed_courses.append(course_data)
            elif gpa_upper == "é€šé":
                # "é€šé" çš„èª²ç¨‹é€šå¸¸ä¸è¨ˆå…¥ GPA ä½†è¨ˆå…¥å­¸åˆ†ï¼Œæˆ‘å€‘å°‡å…¶è¦–ç‚ºåŠæ ¼ç§‘ç›®
                parsed_courses.append(course_data)
            else:
                # å…¶ä»–æƒ…æ³ï¼Œå‡å®šç‚ºåŠæ ¼ç§‘ç›®
                parsed_courses.append(course_data)

        except Exception as e:
            # é€™è£¡å¯ä»¥é¸æ“‡æ€§åœ°æ‰“å°éŒ¯èª¤ï¼Œä»¥ä¾¿èª¿è©¦
            # st.warning(f"è§£æè¡Œæ™‚å‡ºéŒ¯ï¼š{row.to_dict()} - {e}")
            continue # è·³éç„¡æ³•è§£æçš„è¡Œ
            
    return parsed_courses, failed_courses


# --- Streamlit æ‡‰ç”¨ç¨‹å¼ç•Œé¢ ---
st.set_page_config(layout="wide", page_title="å­¸åˆ†è¨ˆç®—å™¨", page_icon="ğŸ“")

st.title("ğŸ“š æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å™¨")

st.markdown("""
é€™å€‹å·¥å…·å¯ä»¥å¹«åŠ©æ‚¨åˆ†ææ±æµ·å¤§å­¸çš„æ­·å¹´æˆç¸¾å–® PDF æª”æ¡ˆï¼Œ
è‡ªå‹•è¨ˆç®—æ‚¨å·²ç²å¾—çš„ç¸½å­¸åˆ†ï¼Œä¸¦åˆ—å‡ºé€šéå’Œä¸åŠæ ¼çš„ç§‘ç›®ã€‚
""")

uploaded_file = st.file_uploader("ä¸Šå‚³æ‚¨çš„æˆç¸¾å–® PDF æª”æ¡ˆ", type=["pdf"])

if uploaded_file is not None:
    st.success("æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼")

    # è™•ç† PDF æª”æ¡ˆ
    with st.spinner("æ­£åœ¨è™•ç† PDFï¼Œé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“..."):
        extracted_df = process_pdf_file(uploaded_file)

    if not extracted_df.empty:
        st.subheader("ğŸ“ æå–åˆ°çš„è¡¨æ ¼æ•¸æ“šé è¦½ï¼š")
        st.dataframe(extracted_df, use_container_width=True)

        if not extracted_df.empty:
            st.subheader("ğŸ“Š å­¸åˆ†è¨ˆç®—çµæœï¼š")
            calculated_courses, failed_courses = parse_course_data(extracted_df)

            total_credits_passed = sum(course['å­¸åˆ†'] for course in calculated_courses)
            
            st.metric(label="âœ… å·²ç²å¾—ç¸½å­¸åˆ† (ä¸å«é«”è‚²èˆ‡æœå‹™å­¸ç¿’)", value=f"{total_credits_passed:.1f}")

            # é¡¯ç¤ºé€šéçš„ç§‘ç›®
            if calculated_courses:
                st.subheader("é€šéçš„ç§‘ç›®åˆ—è¡¨ (è¨ˆå…¥å­¸åˆ†)ï¼š")
                # ç¯©é¸æ‰é«”è‚²å’Œæœå‹™å­¸ç¿’çš„å­¸åˆ†ï¼Œå¦‚æœå®ƒå€‘çš„å­¸åˆ†æ˜¯0æˆ–è€…ä¸è¨ˆå…¥ç¸½å­¸åˆ†
                # æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘å‡è¨­å­¸åˆ†ç‚º0çš„ç§‘ç›®ä¸è¨ˆå…¥ç¸½å­¸åˆ†ï¼Œç„¡è«–æ˜¯å¦é€šé
                display_passed_df = pd.DataFrame([c for c in calculated_courses if c['å­¸åˆ†'] > 0])
                
                # ç¢ºä¿é¡¯ç¤ºçš„åˆ—å­˜åœ¨ï¼Œä¸¦æŒ‰ç‰¹å®šé †åºé¡¯ç¤º
                display_cols_passed = ['å­¸å¹´åº¦å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
                final_display_passed_cols = [col for col in display_cols_passed if col in display_passed_df.columns]

                st.dataframe(display_passed_df[final_display_passed_cols], height=300, use_container_width=True)
            else:
                st.info("æ²’æœ‰æ‰¾åˆ°é€šéçš„ç§‘ç›®ã€‚")

            # é¡¯ç¤ºä¸åŠæ ¼çš„ç§‘ç›®
            if failed_courses:
                st.subheader("ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ (ä¸è¨ˆå…¥ç¸½å­¸åˆ†)ï¼š")
                failed_df = pd.DataFrame(failed_courses)
                
                # ç¢ºä¿é¡¯ç¤ºçš„åˆ—å­˜åœ¨ï¼Œä¸¦æŒ‰ç‰¹å®šé †åºé¡¯ç¤º
                display_failed_cols = ['å­¸å¹´åº¦å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
                final_display_failed_cols = [col for col in display_failed_cols if col in failed_df.columns]
                st.dataframe(failed_df[final_display_failed_cols], height=200, use_container_width=True)
                st.info("é€™äº›ç§‘ç›®å› æˆç¸¾ä¸åŠæ ¼ ('D', 'E', 'F' ç­‰) è€Œæœªè¨ˆå…¥ç¸½å­¸åˆ†ã€‚")

            if calculated_courses or failed_courses:
                if calculated_courses:
                    csv_data_passed = pd.DataFrame(calculated_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰é€šéçš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_passed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_calculated_courses.csv",
                        mime="text/csv",
                        key="download_passed_btn"
                    )
                if failed_courses:
                    csv_data_failed = pd.DataFrame(failed_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_failed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_failed_courses.csv",
                        mime="text/csv",
                        key="download_failed_btn"
                    )
            
        else:
            st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦å…¶ä»–æ–‡ä»¶ã€‚")
    else:
        st.error("ç„¡æ³•å¾ PDF æª”æ¡ˆä¸­æå–ä»»ä½•æœ‰æ•ˆæ•¸æ“šã€‚è«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼æˆ–å…§å®¹ã€‚")

st.markdown("---")
st.markdown("é–‹ç™¼è€…ï¼š[æ‚¨çš„åå­—æˆ–è¯çµ¡æ–¹å¼ (é¸å¡«)]")
