import streamlit as st
import pandas as pd
import pdfplumber
import collections
import re
import io

from PIL import Image
import pytesseract

# **é‡è¦ï¼šæ˜ç¢ºæŒ‡å®š tesseract åŸ·è¡Œæª”çš„è·¯å¾‘**
# åœ¨åŸºæ–¼ Debian/Ubuntu çš„ Docker æ˜ åƒä¸­ï¼Œtesseract åŸ·è¡Œæª”é€šå¸¸ä½æ–¼ /usr/bin/tesseract
pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'

# å°å…¥ img2table çš„ PDF é¡ å’Œ TesseractOCR
from img2table.document import PDF as Img2TablePDF
from img2table.ocr import TesseractOCR

# --- è¼”åŠ©å‡½æ•¸ ---
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    å°‡å¤šå€‹ç©ºç™½å­—å…ƒï¼ˆåŒ…æ‹¬æ›è¡Œï¼‰æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å…©ç«¯ç©ºç™½ã€‚
    """
    if cell_content is None:
        return ""

    text = ""
    if hasattr(cell_content, 'text'):
        text = str(cell_content.text)
    elif isinstance(cell_content, str):
        text = cell_content
    else:
        text = str(cell_content)
    
    return re.sub(r'\s+', ' ', text).strip()

def make_unique_columns(columns_list):
    """
    å°‡åˆ—è¡¨ä¸­çš„æ¬„ä½åç¨±è½‰æ›ç‚ºå”¯ä¸€çš„åç¨±ï¼Œè™•ç†é‡è¤‡å’Œç©ºå­—ä¸²ã€‚
    å¦‚æœé‡åˆ°é‡è¤‡æˆ–ç©ºå­—ä¸²ï¼Œæœƒæ·»åŠ å¾Œç¶´ (ä¾‹å¦‚ 'Column_1', 'æ¬„ä½_2')ã€‚
    """
    seen = collections.defaultdict(int)
    unique_columns = []
    for col in columns_list:
        original_col = col
        if not col.strip():  # å¦‚æœæ¬„ä½æ˜¯ç©ºå­—ä¸²ï¼Œçµ¦ä¸€å€‹é è¨­åç¨±
            col = "ç©ºç™½æ¬„ä½"
        
        if col in seen:
            seen[col] += 1
            unique_columns.append(f"{col}_{seen[col]}")
        else:
            seen[col] = 0
            unique_columns.append(col)
    return unique_columns

# **ç›´æ¥èª¿ç”¨ img2table é€²è¡Œæå–ï¼Œé€™æ˜¯ç‚ºäº†å„ªå…ˆè™•ç†åœ–ç‰‡å‹ PDF**
def process_pdf_file(uploaded_file):
    """
    ç›´æ¥ä½¿ç”¨ img2table + OCR è™•ç† PDF æª”æ¡ˆï¼Œæå–è¡¨æ ¼æ•¸æ“šã€‚
    """
    st.info("ç›´æ¥ä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼ (é©ç”¨æ–¼åœ–ç‰‡å‹æˆ–ç„¡é‚Šç•Œè¡¨æ ¼)...")
    try:
        df = process_image_pdf_with_ocr(uploaded_file)
        if df.empty:
            st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆè¡¨æ ¼æ•¸æ“šã€‚")
        return df
    except Exception as e:
        st.error(f"è™•ç† PDF æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹ç¢ºä¿ Tesseract OCR å¼•æ“åŠæ‰€æœ‰ç›¸é—œä¾è³´å·²æ­£ç¢ºå®‰è£ã€‚")
        return pd.DataFrame()

def process_image_pdf_with_ocr(uploaded_file):
    """
    ä½¿ç”¨ img2table + Tesseract OCR è™•ç†åœ–ç‰‡ PDFï¼Œæå–è¡¨æ ¼æ•¸æ“šã€‚
    """
    st.info("æ­£åœ¨ä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼...")
    try:
        pdf_bytes = io.BytesIO(uploaded_file.read())

        ocr = TesseractOCR(lang="chi_tra")

        doc = Img2TablePDF(src=pdf_bytes, pages="all")

        # é€™è£¡èª¿æ•´ img2table çš„åƒæ•¸ä¾†æé«˜è­˜åˆ¥ç‡
        # implicit_rows: å˜—è©¦è­˜åˆ¥éš±å«è¡Œ
        # borderless_tables: å˜—è©¦è­˜åˆ¥ç„¡é‚Šç•Œè¡¨æ ¼ (å°é€™ç¨®PDFå¾ˆé‡è¦)
        extracted_tables = doc.extract_tables(ocr=ocr,
                                              implicit_rows=True,
                                              borderless_tables=True,
                                              min_confidence=70) # é è¨­å€¼æ˜¯50ï¼Œå¯é©ç•¶èª¿æ•´

        if extracted_tables:
            all_dfs = []
            for page_num, tables_on_page in extracted_tables.items():
                for idx, table in enumerate(tables_on_page):
                    df = pd.DataFrame(table.content)
                    all_dfs.append(df)

            if all_dfs:
                merged_df = pd.concat(all_dfs, ignore_index=True)
                # å°æ‰€æœ‰å–®å…ƒæ ¼å…§å®¹é€²è¡Œæ¨™æº–åŒ–
                merged_df = merged_df.applymap(normalize_text)

                # **æ”¹é€²åˆ—åè™•ç†ï¼šå†æ¬¡çµ±ä¸€è™•ç†ï¼Œä¸¦å‡è¨­ç¬¬ä¸€è¡Œæ˜¯åˆ—å**
                if not merged_df.empty:
                    header = merged_df.iloc[0].tolist()
                    header = [normalize_text(h) for h in header]
                    header = make_unique_columns(header)
                    
                    if len(header) == merged_df.shape[1]:
                        merged_df.columns = header
                        merged_df = merged_df[1:].reset_index(drop=True)
                    else:
                        st.warning("æå–åˆ°çš„åˆ—æ•¸èˆ‡è¡¨é ­ä¸åŒ¹é…ï¼Œå¯èƒ½å°è‡´åˆ—åéŒ¯èª¤ã€‚")
                
                st.success("æˆåŠŸä½¿ç”¨ img2table + OCR æå–è¡¨æ ¼ã€‚")
                return merged_df
            else:
                st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚")
                return pd.DataFrame()
        else:
            st.warning("img2table + OCR æœªèƒ½å¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚")
            return pd.DataFrame()

    except Exception as e:
        st.error(f"ä½¿ç”¨ img2table è™•ç†åœ–ç‰‡ PDF æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.error("è«‹ç¢ºä¿ Dockerfile æ­£ç¢ºå®‰è£äº† Tesseract OCR å¼•æ“å’Œæ‰€æœ‰ç›¸é—œçš„ç³»çµ±ä¾è³´ï¼ˆå¦‚ libGL.so.1 ç­‰ï¼‰ã€‚")
        return pd.DataFrame()

def parse_course_data(df):
    """
    å¾ DataFrame ä¸­è§£æèª²ç¨‹æ•¸æ“šï¼Œè¨ˆç®—ç¸½å­¸åˆ†å’Œä¸åŠæ ¼å­¸åˆ†ã€‚
    """
    parsed_courses = []
    failed_courses = []
    
    col_mapping = {
        'å­¸å¹´åº¦': ['å­¸å¹´åº¦', 'å­¸å¹´', 'å¹´åº¦'],
        'å­¸æœŸ': ['å­¸æœŸ'],
        'é¸èª²ä»£è™Ÿ': ['é¸èª²ä»£è™Ÿ', 'é¸èª²ä»£ç¢¼'], # ä¿®æ­£éŒ¯å­—ï¼šé¸èª²ä»£è™Ÿå¯èƒ½è¢« OCR æˆ ä»£ç¢¼
        'ç§‘ç›®åç¨±': ['ç§‘ç›®åç¨±', 'ç§‘ç›®'],
        'å­¸åˆ†': ['å­¸åˆ†'],
        'GPA': ['GPA', 'æˆç¸¾', 'åˆ†æ•¸'] # GPA ä¹Ÿå¯èƒ½è¢« OCR æˆ åˆ†æ•¸
    }

    actual_cols = {}
    for standard_col, possible_names in col_mapping.items():
        for name in possible_names:
            # ä½¿ç”¨æ›´å¯¬é¬†çš„åŒ¹é…ï¼Œå¿½ç•¥ç©ºæ ¼
            # ä¾‹å¦‚ 'å­¸å¹´åº¦' å¯ä»¥åŒ¹é… 'å­¸ å¹´ åº¦'
            matching_cols = [col for col in df.columns if normalize_text(name).replace(' ', '') in normalize_text(col).replace(' ', '')]
            if matching_cols:
                actual_cols[standard_col] = matching_cols[0]
                break
        if standard_col not in actual_cols:
            st.warning(f"æœªèƒ½è­˜åˆ¥å‡ºé—œéµæ¬„ä½ï¼š{standard_col}ã€‚è«‹æª¢æŸ¥PDFä¸­çš„è¡¨æ ¼æ¨™é¡Œã€‚")
            return [], []

    for index, row in df.iterrows():
        try:
            # ç¢ºä¿è¡Œä¸æ˜¯ç©ºçš„ï¼Œä¸¦ä¸”è‡³å°‘æœ‰å­¸å¹´åº¦å’Œå­¸åˆ†
            if not normalize_text(row.get(actual_cols.get('å­¸å¹´åº¦'), '')).strip() and \
               not normalize_text(row.get(actual_cols.get('å­¸åˆ†'), '')).strip():
                continue

            year_term = f"{normalize_text(row[actual_cols['å­¸å¹´åº¦']])}{normalize_text(row[actual_cols['å­¸æœŸ']])}"
            
            credit_str = normalize_text(row[actual_cols['å­¸åˆ†']])
            gpa_str = normalize_text(row[actual_cols['GPA']])

            if not credit_str.strip() or not credit_str.replace('.', '', 1).isdigit():
                continue
            credit = float(credit_str)

            course_data = {
                'å­¸å¹´åº¦å­¸æœŸ': year_term,
                'é¸èª²ä»£è™Ÿ': normalize_text(row[actual_cols['é¸èª²ä»£è™Ÿ']]),
                'ç§‘ç›®åç¨±': normalize_text(row[actual_cols['ç§‘ç›®åç¨±']]),
                'å­¸åˆ†': credit,
                'GPA': gpa_str
            }

            gpa_upper = gpa_str.upper().strip()

            if gpa_upper in ["D", "E", "F", "ä¸è¨ˆ", "æœªé€šé", "ä¸åŠæ ¼"] or (gpa_upper.isdigit() and float(gpa_upper) < 60):
                failed_courses.append(course_data)
            elif gpa_upper == "é€šé":
                parsed_courses.append(course_data)
            else:
                parsed_courses.append(course_data)

        except KeyError as ke:
            # å¦‚æœæ˜¯é—œéµåˆ—ç¼ºå¤±å°è‡´çš„éŒ¯èª¤ï¼Œæ‰“å°è­¦å‘Š
            # st.warning(f"è§£æè¡Œæ™‚ç¼ºå°‘é—œéµåˆ—ï¼š{ke} - è¡Œæ•¸æ“š: {row.to_dict()}")
            continue
        except Exception as e:
            # å…¶ä»–è§£æéŒ¯èª¤ï¼Œé€šå¸¸å¯ä»¥è·³é
            # st.warning(f"è§£æè¡Œæ™‚å‡ºéŒ¯ï¼š{row.to_dict()} - {e}")
            continue
            
    return parsed_courses, failed_courses


# --- Streamlit æ‡‰ç”¨ç¨‹å¼ç•Œé¢ ---
st.set_page_config(layout="wide", page_title="å­¸åˆ†è¨ˆç®—å™¨", page_icon="ğŸ“")

st.title("ğŸ“š æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å™¨")

st.markdown("""
é€™å€‹å·¥å…·å¯ä»¥å¹«åŠ©æ‚¨åˆ†ææ±æµ·å¤§å­¸çš„æ­·å¹´æˆç¸¾å–® PDF æª”æ¡ˆï¼Œ
è‡ªå‹•è¨ˆç®—æ‚¨å·²ç²å¾—çš„ç¸½å­¸åˆ†ï¼Œä¸¦åˆ—å‡ºé€šéå’Œä¸åŠæ ¼çš„ç§‘ç›®ã€‚
""")

uploaded_file = st.file_uploader("ä¸Šå‚³æ‚¨çš„æˆç¸¾å–® PDF æª”æ¡ˆ", type=["pdf"])

if uploaded_file is not None:
    st.success("æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼")

    with st.spinner("æ­£åœ¨è™•ç† PDFï¼Œé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“..."):
        extracted_df = process_pdf_file(uploaded_file)

    if not extracted_df.empty:
        st.subheader("ğŸ“ æå–åˆ°çš„è¡¨æ ¼æ•¸æ“šé è¦½ï¼š")
        st.dataframe(extracted_df, use_container_width=True)

        if not extracted_df.empty:
            st.subheader("ğŸ“Š å­¸åˆ†è¨ˆç®—çµæœï¼š")
            calculated_courses, failed_courses = parse_course_data(extracted_df)

            total_credits_passed = sum(course['å­¸åˆ†'] for course in calculated_courses)
            
            st.metric(label="âœ… å·²ç²å¾—ç¸½å­¸åˆ† (ä¸å«é«”è‚²èˆ‡æœå‹™å­¸ç¿’)", value=f"{total_credits_passed:.1f}")

            if calculated_courses:
                st.subheader("é€šéçš„ç§‘ç›®åˆ—è¡¨ (è¨ˆå…¥å­¸åˆ†)ï¼š")
                display_passed_df = pd.DataFrame([c for c in calculated_courses if c['å­¸åˆ†'] > 0])
                
                display_cols_passed = ['å­¸å¹´åº¦å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
                final_display_passed_cols = [col for col in display_passed_df.columns if col in display_cols_passed] # ç¢ºä¿é †åº
                st.dataframe(display_passed_df[final_display_passed_cols], height=300, use_container_width=True)
            else:
                st.info("æ²’æœ‰æ‰¾åˆ°é€šéçš„ç§‘ç›®ã€‚")

            if failed_courses:
                st.subheader("ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ (ä¸è¨ˆå…¥ç¸½å­¸åˆ†)ï¼š")
                failed_df = pd.DataFrame(failed_courses)
                
                display_failed_cols = ['å­¸å¹´åº¦å­¸æœŸ', 'é¸èª²ä»£è™Ÿ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
                final_display_failed_cols = [col for col in failed_df.columns if col in display_failed_cols] # ç¢ºä¿é †åº
                st.dataframe(failed_df[final_display_failed_cols], height=200, use_container_width=True)
                st.info("é€™äº›ç§‘ç›®å› æˆç¸¾ä¸åŠæ ¼ ('D', 'E', 'F' ç­‰) è€Œæœªè¨ˆå…¥ç¸½å­¸åˆ†ã€‚")

            if calculated_courses or failed_courses:
                if calculated_courses:
                    csv_data_passed = pd.DataFrame(calculated_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰é€šéçš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_passed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_calculated_courses.csv",
                        mime="text/csv",
                        key="download_passed_btn"
                    )
                if failed_courses:
                    csv_data_failed = pd.DataFrame(failed_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_failed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_failed_courses.csv",
                        mime="text/csv",
                        key="download_failed_btn"
                    )
            
        else:
            st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦å…¶ä»–æ–‡ä»¶ã€‚")
    else:
        st.error("ç„¡æ³•å¾ PDF æª”æ¡ˆä¸­æå–ä»»ä½•æœ‰æ•ˆæ•¸æ“šã€‚è«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼æˆ–å…§å®¹ã€‚")

st.markdown("---")
st.markdown("é–‹ç™¼è€…ï¼š[æ‚¨çš„åå­—æˆ–è¯çµ¡æ–¹å¼ (é¸å¡«)]")
